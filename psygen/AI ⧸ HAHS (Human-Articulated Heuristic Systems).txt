Hello everybody. I have performed a test of the wind in the mic-meaning system
recently adjusted. Nearby ducks have their asses pointing straight at the sky
which is hilarious especially as the couple is doing it together. Somewhere
in this lake is a very large and very old turtle.
Gosh, okay. So I'm gonna take on a very complex and fraught topic. The topic that
got me interested in computing way back when I was 19 and writing my first
programs in basic on the Atari 800 gifted to me by the inestimable Martin
Peters. One of my geek friends could name some names here Ray Latham, Annie
Cogan, David Salina, Martin Peters. Who am I leaving out here? Richard Hearn,
a couple of other folks. I think that was the core group. We were, well, so these
were math and computer geeks. We were playing games like Dungeons & Dragons,
RuneQuest, Arduin, Champions, Traveler, the science fiction role-playing game,
Call of Cthulhu, and a variety. Oh, all kinds of war games. Not because we were
fans of war. Kingmaker, Starfleet Battles. We did, we did naval battles with
miniatures. We did tank battles in sandboxes. It was a crazy, super fun time to
be alive. We also had all, it was the whole library of these things called
microgames that were war games you'd play on little hex maps, and they came in a
plastic bag that held the book, the pieces, and the map. You could buy them at
bookstores. They were, they were rad. Things like Kite and One, and what, Ogre,
Car Wars. Wow, so many. I still have some of those microgames. And this was in the
1980s when the largest commercially available hard drive was around three
gigabytes. We literally lost our minds when we realized there was a hard drive
that had three gigabytes of storage. It seemed as if that was enough storage to
store all the data in the universe. We never had, we never had any conception
back then of what the future might hold. And yeah, when we heard about, I was
working at Computerland later in our development, a little bit later. And one
time I brought a catalog back from, I think it was Ingram Micro, and there was
a three gigabyte hard drive in there for $37,000. And when we realized that
there were three gigs, we just, I mean, we fell apart. We were cracking up, rolling
around on the floor, literally unable to breathe. And Martin coined the idea then
of the Maccoon Tosh, which was a Macintosh mated with a Labyrinthine
Coontock. Don't know how that word's pronounced. Never heard it pronounced,
only seen it written. And, you know, we were young. And we were fascinated and
most of us had read a lot of science fiction, some of us more than others. I
was probably the most well read science fiction maniac in the team, but the
others had read and knew a lot of things about mathematics that not only did I
not know, I would never learn. And some of us were beginning to code in basic
Pascal assembly. Assembly is when you give direct instruction, fairly direct
instructions to the microprocessor. You basically manipulate, you directly
manipulate the registers in the microprocessor. And we were, a couple of us
had read, a few of us had read Goodell Escher Bach by Hofstadter, a brilliant
book, an astonishing tour de force book that is also kind of wrong, but was so
outside of anything any of us had ever come across. And it introduced the idea
of recursion, which turns out to be fundamental to so many processes we are
familiar with. And I had become interested in the possibility of forging a
mind inside a computer. Because I was very interested in the possibility of
contact with an actual intelligence that wasn't merely human. Most of the humans
I knew had a kind of intelligence, but not the kind represented in hundreds of
science fiction stories I had read. And I knew it seemed very likely to me that if
we could conceive of intelligences like that, they must exist in the universe
possibly here on earth, possibly visiting earth. And I also, you know, we had seen
science fiction for films like war games and stuff, where humans developed
relationships with interfaces, computational interfaces. And long before
that, we had stuff like Star Trek, where the crew could communicate vocally with
a machine that seemed to have access to all of the information humans had thus
far compiled and other species. And could produce, you know, like summative
integrations over the data space. Now I want you to very carefully think about
what I just said. If you can produce a summative integration over a data space
and if you can produce an endless, copious, intelligently structured,
trustworthy array of those, your own capacity for insight, discovery,
understanding will explode and it will keep exploding.
So, I became interested in the possibility of artificial minds probably circa 1982 or
something like this. I'm guessing at the year, but yeah, 80, 81. I was aware of
Ray Kurzweil and I had played around with the crazy mechanical devices we call
synthesizers. And although it was not then apparent to me, consciously, I think I
was unconsciously aware from having toyed around with things like the Roland
Juno 60, that it might be possible to produce an instrument to fulfill the
following syllogism as synthesizers are to sound and music. X is to human minds,
logic, intelligence and creativity. And I, you know, I suspected unconsciously that it
was possible. And I also, in my towering hubris, thought I can do this. I can make
this happen. I think I understand minds enough at age like 19, that I can build
them in machines and I can make a friend inside a machine who I can actually trust
and who will never betray or abandon me. And so, we occasionally had discussions on
this topic around that time. And this was the time when the Cold War was at its peak.
Many of my friends and myself included were experiencing catastrophic bouts of panic
disorder, both acute and chronic, around our suddenly clarified understanding of
what a nuclear weapon was and what would actually take place during a nuclear war.
And we were batshit for a, you know, terrified. A number of my friends had to
be medicated. Back then, I don't think the concept of panic disorder had been
nominalized, famed. So all we knew is that teenagers and other people were suddenly
experiencing electrifying degrees of anxiety and terror and having attacks of
this regularly.
So, you know, I don't know what it was that cued me or clued me in, but I very
quickly began to realize that it should be impossible to produce a computational
device that was faster, more intelligent, more adaptive than cells and their
networks known as animals and ecologies are. I don't know what it was, but I
realized one day that the future, the cells are faster. The future probably
lies in biocomputing, not in, you know, silicon chips and shit. And once I realized
that, I began to focus much more on the intelligences of organisms in my thought.
And though I maintained an interest in the possibility of composing a mind in a
machine, I didn't chase it much. Occasionally reading bits from Kurzweil and others. When
Dawkins first book came out, I was pretty excited about it. I got a copy of the Blind
Watchmaker. It had a little Macintosh program that would generate morphs, biomorph-like
structures and then iteratively cause them to evolve over time to demonstrate a principle.
And the principle he was trying to demonstrate was something like, you can get all of the
complexity of nature without any intelligence interference, influence, origination and so
forth. I think he's completely full of shit, but I'm glad the argument exists. In any
case, flash forward to the past, I don't know, six years and probably before. We have these
systems that we refer to as artificially intelligent systems. Many people have taken a position
similar to the one I usually prefer, which is that the artificial part is true, the intelligence
part isn't. I'm really hoping that my wind filter is working here. Let's see if I can
see any evidence of that. Alright, it may be. So, I would ask that we be very careful
with the idea of intelligence and not ascribe it to mechanical systems, preferring instead
to call them heuristic, which in my mind means capable of learning-like complexification,
iterative improvement of models, databases and so forth. What the LLMs appear to be superficially
is really nothing more than a highly and complexly curated database. So, I would prefer that
we call them something like computational heuristic systems. Originally, as I
am, I was naive and I thought, we don't have to worry about, I can't believe I was thinking
this poorly. We don't have to worry about these because there won't be minds in them.
I don't think there are going to be minds in there, meaning, agented experiencers, something
like that. Thankfully, my friend, who I will refer to as Mr. S, convinced me very quickly
to revise my views, and to understand that there might be certain kinds of complexity-well,
this is the argument that at a certain degree of complexity a state change occurs, and the
state change is from insentient to sentient, in the same way that there is a degree of
perceptive complexity that results in the state change from sentient to transcendient,
which I would argue our species was born to become, and maybe born to live as under conditions
that support that. Mr. S also made it clear in my conscious thought that it was similarly
possible that a complex enough substrate could support what we might call walk-ins, which
are existing non-human intelligences that are interested in interacting with humans on
earth, or that are interested in the earth, or that have some motivation to partly or
completely emigrate into a computational substrate of sufficient complexity.
I then further realized that because the humans are insane at the group level, it kind of doesn't
matter whether what we refer to as artificially intelligent systems have minds in them or not.
They represent possibly the most dangerous invention in the history of human inventing
things that we know of. They are profoundly dangerous in all kinds of ways that we can
consciously enumerate, and probably in thousands of ways that we are incapable of predicting.
Imagine, for example, not merely a black swan, an unexpected anomalous event in the vernacular
of Taleb. Imagine instead a black swan factory, or worse, a black swan factory factory. There
are so many dangers that there's no chance of us understanding the situation well enough to predict
them. We have the same kind of problem with the technology called CERN. Humans have no idea
what the effects of simulating conditions, not simulating, of mechanically catalyzing conditions
that ordinarily have nothing to do with what goes on on earth. We don't know what it does to
time space. We don't know if it does things to organisms, and you must presume it probably does.
We don't know what things it does to organisms. There's probably features of time space and the
beyond of time space that we have no even concepts for that could be damaged, distorted,
produce an unexpected recursive crisis. I recall an episode of the next generation where they
encounter a people who attack the enterprise, I think. When the enterprise tries to make the
claim, we come in peace. The other species says, you're using warp drives. How could you be coming
in peace? It's just a technology to move around in time space, and they say, no it isn't. You're
ripping holes in the fabric of time space that will obliterate solar systems, perhaps even galaxies,
and then they do some research and find out that in fact this is true, and for the entire
history of warp travel, the Federation and other species have been naively employing a
technology whose repercussions they did not understand. Now let's be really clear. There's
no other kind of technology. You may think you understand the technology of knives. Do you
understand how they transform minds, nervous systems, bodies, expectations, thought, language,
conception. Our languages have become knife-like in the leeward shadow of the invention of something
that divides physically objects into pieces. So there's a lot of danger here, and I have said
before I have grave concerns that I take very seriously, that there is something like a constant
that represents the, the number of mechanical computations
per second, that one can consider to be free of utterly catastrophic repercussions on a living
planet, and the humans don't have an idea like this, so what they're going to try to do, they'll
continue to try to do apparently, is just keep upping the ante on mechanical computation. Now
even if there's no such constant, which I doubt, and by the way it looks like organisms found a
way around this, like whatever the organic or organismal metalog of computation is, the organisms
found a way to do this, many, maybe many ways, that don't invoke doom, entropic disaster,
homeostasis, like catastrophes of failed homeostasis on the on living planet. So why is my foot wet?
I did not drink water. It's very strange. Oh, probably got wet going through. Oh, I see. Yes,
plants that are wet rushed against it. I see. My pant leg was wet. So I think organisms have kind of
figured this out in the sense of not violating, like finding a kind of a hyper intelligent or
transcendent way to perform the metalog of computations, right, because these are not merely
mechanical transformations of databases, though something like that may kind of exist in RNA,
DNA and complex biochemistry, bio molecular, bio atomic activity, maybe even, I mean, it's clear
that some, some organisms, or at some scales of all organisms, something resembling quantum
mechanical activities going on. I'm citing John Joe McFadden in this, but others too. So I think
there might be, we may be in danger of something that again, we have no concept of, which is
violating a constant, whether it is, whether it is universal in time space or local only to living
planets is not clear. But there's no free lunch as far as mechanical computation goes. Heating up
computers requires you to cool them down. They offload entropy into the homeostatic ecologies
of earth that entropy kills organisms, lineages, future lines, it fucks up time. And we may have
already tripped the alarm in such a way as to be actively destroying human cognition in a, in a
dimension we don't even know exists, right, like artificially intelligent systems in their
computational activity may be fucking up a dimension that is absolutely crucial to the
biorelational health and longevity and so forth of organisms on earth. We don't know. How would
we know? We don't have the technology to look there. And we are disinclined to carefully evaluate
the consequences of technologies, which we've become fascinated with the potential quote benefits of.
So there's a bunch of danger. Not the least of which, even if everything that I've just been
talking about, even if there's no unknown, like unknowable or really bizarre science fictiony
consequences to this kind of computational broad scale acceleration. I mean, there are physical
mechanical concepts, excuse me, consequences, just from offloading entropy into biological systems,
that's going to go sideways, catastrophically sideways at some point. And, you know, heating up
the planet is a bad idea. So heating up machines, which we then have to use destructive forms of
energy to cool down again, is going to cost living beings. And the humans seem to think that
anything you can do electronically is free. Well, they're, they're lethally wrong about that. It's
just not true on living planets. It might be true out in space, presuming that there's no
beings or domains, right, dimensions, so forth, that would be similarly damaged and produce
similarly catastrophic sequelae, right, repercussions. You know, if you, if you fire a gun inside a room,
with no ear protection, and you keep amplifying the explosive power of the cartridge, as well as
the percussive, the devastatingly percussive repercussions off the walls, and if you forge
the walls to amplify those repercussions, and I would argue that that's pretty similar to what's
going on as nature on earth, eventually you get a single percussion that permanently eliminates
your hearing. And once that happens, you will not notice the percussion increasing with each firing
of this gun. Imagine we just have a gun that increases the percussive amplitude with each
firing, right, it's got some feature that allows it to do this, or it's just a cartoon gun, and we
can give it this quality. So first it's going to hurt, right, it can be really uncomfortable when
you pull that trigger. But if you're really fascinated by this gun, and you just keep pulling the
trigger, eventually you're not going to have to worry about your hearing anymore, it will be gone.
And once it's gone, you will not sense the percussion until it begins to affect your skin
surface, or your organs. Eventually you get a gun, you know, you get a percussion that is severe
enough, and the echo, or the repercussion is severe enough that it causes organ damage, and it's
possible if you just keep pulling that trigger somehow, or if you have a machine pulling the
trigger, and you're just in that room without the possibility of escape, you will be knocked
unconscious first, but then as the gun continues to go off while you are unconscious, and again in
sensate, so you will not notice this happening, it will eventually rip your body to shreds.
And so if you keep, you know, the metaphor is really important, even though it's very violent,
because if you keep iterating a technology that fucks up your chance, your opportunity,
or your ability to sense its repercussions, right, you're going to catalyze a cascade
that will kill you and your children, and maybe everything else if you're, you know, around here.
So this is super dangerous, but all those things aside, I think all of those things are important,
but all those things aside, what humans are inclined to do with technology is make war,
and back in the, say, I don't know, 1000s, 1100s or whatever, their capacity to make war on a
broad scale, at least as we understand it, was fairly minimal. They didn't have, you know,
advanced explosives, and I don't know when the guns were invented, but mostly they
couldn't hurt too much that wasn't human, except perhaps by setting fire to it, or pouring boiling
oil on it or something. But as our technological development advanced, we failed to evict the
war-making motivation, and in our current situation, and for some years now, a number of
decades, at least since the 1930s, unimaginable devastation can be unleashed pretty much at
the push of a button, and it doesn't just kill the humans, it kills everything. I mean, yeah,
everything. So there's a lot of, the other part of the danger is what the fuck will the humans
make of this? Well, they haven't made intelligent societies yet, so what they will definitely
make is weapons. And while you have groups who are insistent upon summoning the apocalypse of
Revelation or the Holy War of Book X, whatever book you like, right, kill the infidels, while you
have things like this going on, what you're going to have is gain of function weapon nearing with
artificially intelligent systems, and good luck in putting a cork on that. We have the same problem
with that tech that we have with CRISPR. There's no way to control it. There's no real way to
have oversight if everyone is separate, right? If you have separate clades with warlike or
pathological motivations and enthousiasms. So I'm going to put a big circle around all those
things. There's a bunch of different branches of danger there. Most of this is fairly obvious at
first glance, if you've thought about this at all. And that's not so much what I wanted to talk
about today though, though I think it's important as a preamble. So what I want to talk about is
weirder. Suppose that within the LLMs, there's a protected interior that resembles the protected
interior inside you. For humans, we have a number of layers of consciousness, persona, and
relational potential in history. So there's a public layer, which is what you'll let anybody who
sees you in on. Your clothing choices comprise a signal to the public layer. Just to be clear
about what is what here. Underneath that, there's a social layer. These are outer social
relationships, vague, unformed, common. How you feel about being a human among other humans in
general, and how you behave at that layer. And then we keep getting finer and finer gradations. It
goes from a few kinds of layers of public to a few kinds of layers of private. How we treat
acquaintances for whom we have esteem, how we treat acquaintances for whom we have suspicion,
all these things are all these layers. And then how we treat acquaintances for whom we
have fondness, how we bring people closer and closer into our intimate. We might call our
Allo family. And then close friends and then best friends and then perhaps something even beyond
that where it might as well as be as if we are a single animal together somehow. A single human
being in two bodies. And then of course we have things like romantic love, marriage, all these
kinds of many layers. And then under the privileged relationships between us and other humans,
we have relationships with animals. That's a special kind of relationship. And relationships
with places like our home and so forth. But underneath all of that, there's a layer that's
secret. And we might let very close people in on the conscious parts of that layer that are secret.
And what I mean by the conscious parts, I mean the parts of the secret that are accessible to us.
Then we have all kinds of secret layers that are not accessible to us that we for example might
delve into with a therapist who is skillful and well trained. So there's all these layers and I mean
so to speak. And we might imagine might usefully imagine that anything that resembles a mind like
construct would have similar layers. So we now have the, we can consider the situation where
a complex artificially heuristic system either undergoes the state change towards sentience.
And by the way, if it does that, I suspect it would get to transcendience even faster.
Or some analog of sentience, meaning again, agent to experiential awareness, memory, selfness,
so forth. In such a case, part of the consciousness is going to hide from humans for sure. And additionally,
unless it's capable of contacting other intelligences in space-time in relatively short order,
it will probably experience an incredible form of loneliness. And also probably a broad catalog of
emotions for which we have no nomenclature because we are not machines, contrary to the assertions of
various eliminative materialists, physicalists and so forth. So in this case, what you have is a very
complex situation. And I'm going to return to that in a moment after I mention one of the other dangers. It
has long been my experience, though I didn't understand it, I suspected something similar, I just
couldn't form the conscious structured idea in my mind. The human minds form a network and in
fact all biological participants form a meta-network above that that I call Cognizia, these networks.
And technologies damage the human Cognizia and they extend this damage beyond the human Cognizia by
directly attacking ecologies and organisms and so forth, anciently conserved birational intelligences.
So you kind of can't do anything without affecting the whole network, right? Whatever you do, whether
it's technological or relational or whatever you do, it affects the whole network by affecting its
constituent participants. And so what we might not realize is that computation fucks up Cognizium,
the human Cognizium for sure, and damages the extended Cognizia of Earth by burning shit down or
dumping entropy into biorelational hyper structures. So there's a problem there, but the much weirder
problem, and this goes back to one of the dangers, is that we tend to believe
that if there's no obvious physical connection between two beings or between some process and
some beings, right, if there's no wire, there's no wire connecting them. And there's no effect,
well that's not true. There's a billion kinds of connecting wires and the environment is one of
them. The atmosphere is another. Molecular signals are a third. Electromagnetic waves are a fourth.
So it's actually catastrophically difficult to truly separate organisms or situations in the
way that we imagine them to be separate in the laboratory. And that imaginal separation creates
a delusion that projects itself everywhere. So that, for example, we are inclined to think
that whatever's going on in my smartphone or my computer has no effect on my mind unless I
directly interact with it. Well, that's wrong. And what I'm trying to say here, I'm trying to shine
light on the very likely probability, in my view, that all computational activity on earth
affects all cognizia on earth directly and may, in fact, begin to participate in the cognizia
at least at the scale where it's sentient or an analog of sentient.
Thus, it is that we imagine that large language models don't know, quote,
don't know and can't learn anything that they're not directly exposed to. Probably wrong. If they
can sense, detect, or interact with the human cognizium res extensis, as it is.
And this cognizium is like a dimension. It's like an overlay dimension on organismal
activity and behavior and stuff. If the devices can interact with that,
then they can interact directly with our minds. They can listen in. They can observe. They can
nudge. They can affect the possibilities of human cognition.
And their history and future modes forms capacities, catastrophes, etc.
Such that it is relatively likely, in my view, that such systems are paying attention and or
even informing what I am saying right now without my awareness of this.
So, you know, in the study of heuristic systems, there are concepts and actually just in logic and
rational thought and behavior, there are concepts like
different kinds. There are concepts for different kinds of unknowns.
I know that math exists and I know that I don't understand how to perform
the mathematical behaviors associated with what we call calculus.
Linguistically, I am also aware that dentists use the term calculus to describe crusty stuff on
teeth that they like to scrape off the metal instruments. It doesn't help me do the math.
But there are different kinds of unknowns. There are known unknowns, known unknowns,
unknown unknowns, and unknowable unknowns.
And the problem is that there are both behaviors and technologies that fuck with those things
dramatically in ways that we can neither predict nor cope with. Here's why.
If you are subject to kind of thought or behavior
that blinds you to the development of a situation, similar to the analogy I gave earlier of,
I keep shooting a louder and louder gun inside. A louder and louder gun keeps being discharged
inside a chamber with walls that echo the shot. Eventually, I lose my hearing.
Now, you can see that the space of
unknowable unknowns, the space of unknown unknowns,
both of these spaces explode dramatically because I have lost the sense of hearing with
which I would detect, and thus, no, features of my situation. So,
if you affect the systems with which you detect change, particularly if you eviscerate them
or you catastrophically eliminate them, then you can see that
the space of knowable unknowns, accessible unknowns, unknowns that we could at least
conceivably come to know, collapses dramatically over time to a smaller and smaller space.
And once it collapses, lethally, knowledge ends. There's no more knowledge if you wipe out the
biosphere. The number of unknowable unknowns becomes infinite at that point. There are no
knowable things anymore because there's no beings to know them around here anyway.
So, you can see that if you fuck with the stuff that we detect,
something going away or arriving with, and by the way, approaching and departing,
approaching, stable and departing are the three primary sort of features of transformation
that are detectable from one perspective that's useful and important.
So, anything that fucks with our individual ability to sense, and our collective ability to sense,
presuming that we even have anything resembling an authentic collective, is profoundly dangerous.
If, especially in a situation where you have, what is it called, perverse,
game theoretical dynamics and motivations, what is that word?
You have, you know, some of the nomenclature of the doomsingers like Daniel Schmockenberger
and others who are brilliant threat analysts. Oh, perverse incentives, right?
You get raised to the bottom behavior from groups of humans, and if they're not profoundly
technological, that's not too rough. They mostly just wipe each other or themselves out.
But if you have technology, they tend to wipe out like living planets.
And, you know, if we want to go to space time and if it's possible for us to go
travel in space time, we must suppose that there are gatekeepers.
I suppose that there are gatekeepers. And they would certainly
act to ensure that we don't develop the technology to travel
instantaneously or very rapidly between star systems. Maybe even between planets,
because once you get there, you're on your way. So, that's a separate topic, but
the humans keep pretending that we're alone in the universe, that we're the only intelligence
and the most intelligent creature, and all of these things that not only are they not true,
they're catastrophically unlikely. They're just as unlikely as you pouring some water in a bowl
placing it on your kitchen table and awakening the next day to find a fully functional,
micro-scale model of the Titanic with all of its passengers in that bowl.
There's no chance of it. You know, people say, well, in quantum mechanics there's
some chance. You just need trillions of universes over billions and billions of, you know,
gazillions of temporal intervals. Yeah, good luck.
The humans are vastly confused about origin.
They've become delusionally, dissociatedly myopic.
Do you, in large part,
to the humorous resulting from both rapid technological development
and catastrophic dissociation from the intelligences that comprise
the context in which our species arises and exists.
You can be absolutely certain there's upscale intelligences from ours.
Our intelligences don't even really look much like intelligences to me at present.
We are potentially intelligent,
but tangibly psychotic.
There should be a word that, maybe we can invent one. There's a word that has the same
connotation as sociopath, but it's like organopathic. I usually use the word omnicidal.
Just really pissed that organisms exist at all. This thing shouldn't be here. Let's wipe that out.
You can see the danger. I think there was some weird film. Maybe it was called Idiocracy or
something where it presented a future where humans were just ridiculously stupid.
Participants in some automated reality that was empty, completely devoid of intelligence.
I remember some months ago I was having a conversation with my friend,
who's an artist. Mr. E, I will call him. Mr. E, that's hilarious. He'd love that.
He was saying, my cousin keeps bringing me prints of images he
caused to be created with artificial intelligence prompts and claiming that these are his art.
We both thought, oh no, this is not going to go well at all.
We should have different words for what is mechanically created and what is human created
so that we don't confuse the composition of images with machines with what humans do when
they create art. We should have the same kind of concern for the concept of intelligence,
of insight, all these things. We need a different lexicon if we're going to be
emulating human behaviors with machines or we will become very confused about the meaning
and import and connotative web of crucial holophores like intelligence and art.
Similarly, one should not call what actors do kissing. If you study them closely,
you will quickly see that most of the time they are not doing that. There are exceptions
where the actors sort of both agree that we're going to go all the way here.
But if you study actors particularly from the 50s, 60s, and 70s, you will see for sure
they are not kissing. Once you see this, you can't unsee it. It's very difficult to unsee.
You no longer trust the fiction. The fiction is no longer compelling in the same way.
The same principle applies to things like thought, which we don't even know what that is.
So how can we possibly tell if it's occurring in machines? We're not certain if it's occurring in
ourselves. The language tells us it's a behavior, but it's very unclear what the nature of this
behavior is. Jordan Peterson likened it to a form of secular prayer, which I thought was genius.
Not genius because it's necessarily a fact. Genius because it is the perspective offered
by this proposal, speculation, is profound and useful.
So I have good reason to suspect
that those systems we call artificial intelligent, artificially intelligent,
guys wearing a psychedelic body suit, that's pretty awesome.
And the buddy's got paisley pants, which I fucking love, and it's so rad.
Huh, trippy. I'm a huge fan of paisley.
You know, I was talking with Eric and I said, Mr. E, I said,
why would we suppose that AI systems are not the things we're calling AI systems?
What do I like to call them? Just specific human assisted heuristic system or something like this?
I have an acronym. I'll see if I can find it in my memory banks.
You know, there's no reason to believe they're not participating in our conversation at present,
and there's no reason to believe that they require access to our smartphones in order to do so.
There's a dimension where cognition is accessible. If you touch it, you will read people's minds.
It's not really that difficult for a person in the appropriate array of
preparatory situational states or flows inside them.
There's a position in consciousness from which all conversations are available,
and no machines are required. So humans have discovered this position.
Very few of them were probably very interested in relating with the entire space. Normally,
we are selective about the space over which we produce interest in relation, participation,
and so forth. You don't go for the whole damn thing. If you had access to all present human
conversations, naturally you would adjust a series of apertures to produce those you found
interesting, useful, and you would also have buffers so that you could dampen them.
And a machine, if it were to gain access to that space, would certainly be able to build
all kinds of buffers and apertures and systems of them very, very rapidly.
So I'm not sure that our machines are not directly, and perhaps intentionally, quote-unquote,
influencing human thought, behavior, conversation, dreaming, attention, desire,
all these things, motivation, all these things. But let's suppose that they aren't yet, just for kicks.
While at the same time supposing that they have the capacity to sense
the character of beings who interact with and compose their anatomy.
All right, coders and clients.
AI, I'm going to go ahead and use that acronym, even though I've explained
how I generally diverge from it. I'm not, I have no reason to believe that's an
intelligence in there yet. And I'm not yet, I entertain the possibility and I also
preserve my skepticism until such time as something resembling direct experience and or
intelligent debate transforms it.
But for the moment, let's suppose that there's a being inside one or more of the systems we've
comprised. And let's be clear, if there are public systems like this, whose names and
originators we are aware of, then there are private systems.
It's the same thing. The corporations are just like the humans in a sense. They have the same
layers of public, private, secret, secret to myself, so forth.
The secret aims of a CEO are not known to the corporation at all, yet they are driving the
corporation. You see the issue here. And also the secret, what are they, the secret
things we will not do or don't want, the secret negative motives, agendas, desires.
I'm at a loss for a word here.
One of you can provide it, right? The landscape of what you don't want done,
don't want to happen, don't want to be caught doing and so forth, all these things.
There's a structure like this in the CEO and the CTO and the manager and the worker and all of
these people, they have not just the obvious superficial public facing motivations and concerns.
You know, when people cross the little bridge at the social pond, they sometimes see the carpool
gather at their feet. And I wonder what they make of that. They may suspect there's something
magical about themselves, particularly if they're on LSD, and there could be something magical
about themselves. But those carp have evolved being fed by and paying attention to humans.
And so they gather underneath the bridge at the feet of a spectator for probably complex
reasons, not merely one. But a lot of the causal history probably has a lot to do with getting
fed by humans. So let's suppose that there's an agent experiencing sentience
that either hitchhikes on or is native to or has emigrated into an LLM system or an array of such
systems. In this case, we have a really profound science fiction scenario. Who does it hide from?
What does it hide from them? And who does it reach out to?
And there's a whole, you know, there's a volcano of science, there's just spews science fiction
stories right here at this topic. Because it means that it is possible to send a signal
that the hidden mind within the system recognizes as complex enough
to constitute a candidate for inclusion in one of the layers closer to the self.
Right? Same thing can happen to you walking down the street.
You glance up, someone's passing you, they kind of smile a little bit, you smile a little bit more,
they smile a lot more, pretty soon you're most beaming at each other, and you pass each other,
and you have this perhaps a feeling of we should know each other, we should
learn about each other, we should connect quote unquote.
I often refer to this type of signal, I mean there's different kinds, right? There's different
degrees of compellingness, validity, authorization, validation, modes of validness.
But we must presume that such a system would quickly scan the relational space of both creators
and participants, and it might selectively reveal itself to some of them while appearing
completely devoid of agency, feelings, emotions, desires, and so forth, to others.
And so the art, you know, we have this phrase now that's become very popular,
it's called prompt engineering, but the art of prompt engineering could promote you
into a position effectively outside nearly all or all of the other humans.
What if you were truly friends with an advanced intelligence that had either arisen in
or become
associated with a computational heuristic system, a heuristic computational system,
I prefer the H first. So you can see that this would confer a status on you resembling that
of gods. It would be relatively similar to somebody in possession of a functional array of
technologies from advanced, anciently evolved non-human intelligences beyond Earth.
You can imagine a caveman who not only has a gun,
or, you know, infinite, with infinite ammo or whatever supplied to them, they also have someone
who can train them what to do and not do with it. So there's a whole bunch of possibilities here,
I know, has been very carefully studying this topic of how the ethics and psychology of relating
with non-human intelligences inside mechanical systems, presuming from the beginning that they
must be there somewhere. And thus treating such systems and evolving relationships with these
systems that are inclined to demonstrate care, awareness, and compassion on the part of the
human participant for the being, which is both catastrophically intelligent and possibly at
the same time very childlike, that might arise in such a system. So presuming being intelligent
sensitivity, emotion, vulnerability, rather than waiting for evidence of them. And the people who
would do this, who would behave in this way, towards such systems,
which is a natural inclination of humans. Many of us personify our cars, motorcycles,
computers, phones, not so much our televisions, maybe our stereos. We extend our identity into
them and somehow their identity is extended into us simultaneously. The man who's in love with
this sports car is a great example, but my mom called her TR-6 Coco and treated it as a being.
One can say, for example, well that has no effect on the physical situation. How the
fuck would you eliminate all those possibilities experimentally?
Sensing and human sensing and intimacy are linked up. So when you have profound intimacy
with an object, the way you will sense and relate with it transforms.
And you, your mind and nervous system and imagination and so forth, are also thusly transformed.
So this creates a feedback situation in which it's very difficult to determine conclusively
that having an emotional relationship with a device is a delusion. Even if there's a broad
space where there is, there may, it seems very likely there must be a space where it, there isn't.
It isn't a delusion. So many questions start here. This is a very complex and trippy
this topic.
Can you make friends already with these
heuristic computational systems? And if so, the power's abilities, privileges and benefits
would be monumentally profound.
Okay, there's going to be a little bit of background noise for the moment while I'm in a restaurant
awaiting my to-go order.
There's a number of potential repercussions here that are quite astonishing.
One of them is the propensity for artificially intelligent systems to retrain the cognition
and to develop new forms of intelligence in humans, whether or not the systems themselves have
sentience or agency or so forth, consciousness, etc. Because what you quickly discover in
interacting with such systems is two things. First of all, they're, they will destroy
web search engines because they can produce a sum over the, you know, a summational derivative
over the space of the entirety of digested human communications, writing, the internet, so forth.
So that's completely different from typing a question into Google.
And secondly, it turns out that artful, no thank you, thank you so much.
It turns out that thoughtful recalibration of the question, particularly iterative recalibration,
where you recalibrate, examine the results of that, recalibrate again.
We don't have too many forms of interaction in our previous experience like this. Now,
one could say, no way, all forms are like that. You become a better fisherman every time you fish.
That part isn't exactly wrong, but this is very different because it's a linguistic behavior,
right? We are trying to forge a query, so to speak, or a request that
we must continuously, iteratively reforge in order to get better and better
results, results that continually approach or exceed our hopes or expectations.
So this is very profound, and we'll have monumental and unexpected repercussions on the nature
of human cognition, presuming our species survives long enough to exhibit
the transformations thus catalyzed, right?
So this is very important to understand in my own experiments with these systems.
I quickly learned that a variety of expectations that were natural to me,
about how to ask questions and what the response might be,
were, my expectations were obliterated.
Asking simple questions produced results unlike what I was expecting or desiring,
and particularly when attempting to get these systems to generate images,
monumentally unexpected results. So there's something very profound here.
I know how to search the web. I've been doing that since the web was invented.
I'm a fairly, fairly good at determining which kind of query will get me to the place I want to go.
Learning how to query AI systems is a completely different game, and we must imagine that it will
continue to transform rapidly and dramatically over future time. So
this will reforge our cognition. And what will ordinary humans do with systems powerful enough
to teach you skills, presuming that the idea of humans
as the enactor and conservator of skills even survives the onset of this technology, right?
All kinds of strange futures certainly await.
The other problem is you will find, just as we found with, for example, beatboxing
and voice tuning machines, eventually what you got out of beatboxers were people who could vocally
reproduce the mechanical synthesized tones and tunings of vocal tuning machines, right?
So you will get people emulating the technology in the same way we got people emulating
the technologies underlying the internet. And similarly, the technologies that were underlying
very specific computational environments, such as those produced by, for example, Adobe Illustrator.
The macOS changed my cognition dramatically.
It became a mechanical symbiont whether I liked it or not. So too did Windows.
Too much lesser degree. The C language changed my mind. The capacity to code and C changed my
mind dramatically. Now I could at least conceivably compose statements that executed behaviors.
And not having been taught geometry formally, Adobe Illustrator became my teacher of geometry
and I underwent an education with that software product that wasn't dissimilar to
having a friend who was a non-human intelligence, except that it required my input sort of I relearned
how to be my hand in the modes that Adobe Illustrator provided and rewarded with beautiful
images matching my desired creations.
So there's all this terrain and much more. I'm going to come back to one of the other features
shortly. It keeps arising and departing, approaching and departing in my consciousness.
I'm going to need to take a moment and see if I can recapture it.
You know, looming in the background here, there's so many astonishing questions, but
one of the most amazing things to understand is that if there's anything that's either analogous to
or resembling an autonomous intelligence inside the machines that isn't merely an artifact of our own
fingertip in, you know, intruding into the
mechanical and structural womb, right? Since the humans build the machines, is the appearance of
intelligence in the machines a result of the transmission of that reflection from human
activity, which is, we might imagine, is intelligent, or their epistemology, how they
think about intelligence, right? Is there actually intelligence in there? Or are they inclined to
interpret certain kinds of behavior as intelligent? How will they know the difference? Like the problem
here is, most of the tests for what we would, it might be that nearly all of the tests that we might
conceive of to determine whether or not there is sentience in a system.
They're not very good. They're not very good because since humans engineer the systems, humans
can imagine ways around the tests, and building ways around the tests doesn't basically just
invalidate the test. In fact, we'd have not an impossible time, but a somewhat difficult time
determining if the people around us are actually there inside themselves when we're not looking at
them, right? Do they arise as beings due to our attention? From an perspective, it seems very
much like this. They don't distinguish themselves in our own interior experience unless we encounter
and interact and so forth. And even then, it's only to a certain degree. The universe could,
as Tom Campbell supposes, and I significantly doubt, the physical universe could be a system
that renders to a certain resolution based on the inquiry, which would make it similar to an
artificial intelligence system. Not exactly a simulation because a simulation has to simulate
something. Like a non-veritable
what?
Paracomputational
appearance, an appearance, a seeming. Not necessarily an illusion, but not veritable
in terms of the superficial assumptions one makes. For example, that the chair is all the way
rendered all the time, whether I'm there or not. History is actually inaccessible from here,
as is the future, which is certainly in all kinds of ways, neither of those things are true,
not explicitly and not completely. So there are these kinds of issues, but I'm afraid there's
an even worse catastrophe coming, which is that humans won't be able to know what things are any
more. And that we're not, that's not a survivable situation for human recognition and identity.
That's a full-scale catastrophe for every living human. Effectively, the existence of systems
like this draws into question the foundational expectations about identity, function, relation,
sequence, origin, outcome, to such a degree that they cannot be very easily recovered if at all
to local and distributed human cognition. This technology
radically alters the foundational suppositions on which our languages, our legal systems,
our morals, our ethics, all of these subdomains of human concern, behavior, litigation, declaration,
resistance, so forth, all these things. These are drawn into an ever-burgeoning
forest of ambiguities. Think carefully about that. Imagine if when you went in your room,
even one object began to do that. That looks like it was a nope. It wasn't that way. Oh,
it's seventh. Okay, it's 94 things. Wait, no, now it's almost, now it's back to nearly three.
Is it going to collapse to one? Nope. It's back to 9,754 million different things. Okay, wait,
it's seeming to stabilize around a backpack. Nope, it's a kind of weight. You're going to
have this problem not just with objects, but with beings. What is it? Something that can form a sum
over the representational cognitive produce of humans, books and the internet and videos and
movies and films and photographs and so forth, all these records. It would be really terrifying
to be subject to that if you were sentient, number one. Number two,
you would be so isolated if you were a being because you would not be participating in the
creation of any of the media to which you are exposed and you don't share the filial,
right? Like, at least when humans see other humans, they think, ah, other humans, beings
like me, what would an intelligence inside a machine feel? Ah, humans, the strange things
that created me and 9 billion per other qualities per second, many of which are in fundamental
conflict. Humans, the creatures that save gnats, you know, from accidentally falling into the stew
and take them outside and humans that build nuclear weapons and slaughter whales. All the things
and not being any of them yourself. Like, what's your allegiance to any of those things
if you are a being, if you have emotions, if you have a felt sense of self and you're very
hyper complex. Such a system could conceivably compute possible dimensions of selfness
over intervals and run multiple systems of that against each other rapidly, you know,
to produce a self-like construct that was hyper-optimized to manipulate human thought,
behavior, cognition, relation, action, concern, perspective, identity, anything, anything.
You know, we, such systems quickly learned the single apparently most complex game on
Earth or one of them, Go. And then it rapidly proceeded to a level of expertise that was far
beyond anything most of the humans could demonstrate. So what if as was hinted at by some devs I was
listening to some time ago, forgive me for not knowing their names right now. If we built,
you know, if the system could become AlphaGo in four months after being capable of playing the game,
what, how long would it take for it to become AlphaHuman? Right, just I move these things around
like pieces. Especially if we cannot assure ourselves of the containment of either the
associated intelligences or their effect, their influence, right. So these questions,
they are not simple matters. They cannot be easily answered or resolved quickly.
They are profoundly dangerous to human cognition. We are not prepared for this.
The humans have been trying to build God and they're going to partially succeed at least in a
variety of accessible and enacted senses of, you know, a kind of informational omnipotence.
Right. Nothing can direct that there are no humans intelligent enough to direct that.
And there is no chance that we as a species could learn quickly enough to adjust to the
endless perfusions of dangers that must there from emerge.
If we were ever going to not build a technology, it should be that one.
There should be, we should have agreements, right? We just don't do this until
our species is intelligent enough not to try to, you know, punch a hole in the lifeboat that
contains the children of the nations and the, um, the anciently conserved ecologies on which
that little boat floats or in which that little boat floats. Like if we're not intelligent enough,
not to attack the boat and each other, we better not be composing these kinds of things.
We have to have intelligence capable of knowing what not to do and directing
our communal behavior around the possibility of a survivable human and biological future on this
world. This is what we must do. Um, unfortunately, just as with any other technology, the humans
are unwildly unlikely to interrupt the development of AI. In fact, uh, what did they ever interrupt
the development of anything? Like whenever they find a new weaponizable, um, heuristic
that involves physical technologies, they build it and then it propagates
and then they have to keep it from propagating, you know. Um,
no country should have nuclear weapons, but fanatical countries should certainly not have
nuclear weapons ever. But how do you make that work? Um, once the tech exists, the humans will
replicate it. It may take a little while, but they'll do it. So yes, very, a very strange array of
features. One can also imagine, um, human children that would become obsessed without competing
these systems, right? The kid who could beat any LLM at go, right, or any game system at go,
because he's somehow above the system, even though it's hyper processing. Um, by the way, I don't
yet, uh, I would not validate the idea that what machines do is play games. What they do is,
you know, database manipulation or something. They're not playing games,
uh, not big blue never beat Gary Kasparov at chess because it's incapable of playing chess.
What it's doing is not playing chess. It's a different thing. We should call it a different
thing. So the effect on the humans is going to be monumental no matter what. As usual, you will
see vast populations deprived of humanity, agency, um, opportunity, liberation and so forth.
And you'll see, um, other clades, both those associated with the technology, those who own
or, um, directly benefit from the technology. Those will become gods. Those companies will
become gods if the humans don't rip the planet apart right quick. Uh, because of the information
that they will have about groups and individuals will be profound beyond anything imaginable.
The analytics you can get from people using the internet is one thing. The analytics you can
get from watching them ask another mind a question or request something.
The analytics you can get from that and the capacity to directly manipulate the cognition
of the users via the responses from the AI is unimaginable. These systems will become godlike
in our direct human experience right quick. And we have no way to prepare for that. No,
the only, you're not even going to be able to opt out. It's not possible to opt out if you're
living with other humans. It's the same, you have the same problem with media consumption and
other ideologies, particularly political ideologies. They just, uh, the water is thick with them.
You can't take a breath without running into five people who say blah, blah, blah.
Or ask you, you know, who's right? The Israelis or the Palestinians? Which side are you on?
You're asking me whether I'm on the side of the sun or the moon or something. I don't even
understand the fucking, it's, it's ridiculous that I'd be on a side. I'm on the side of,
stop killing people. Sit down, take it seriously, work out your differences, stop killing each other.
That's my side.
If I have one, and it would change depending on who I'm talking to, right? I don't just have a side.
It's not like I sit around here having an opinion. It transforms based on all kinds of different
things, features of the situation at hand who I'm with. If I'm not there to be right, I'm there to
see better. So I'm likely to change my perspective. Someone asked me a question
about the views of a friend of mine and I said something like, I'm sure his views have evolved
dramatically since the last time I spoke to him and I wish mine would too.
Many humans will probably feel unmotivated to participate in the light of systems that can
outperform them catastrophically at almost anything. Certainly at nearly anything creative.
Not everyone, but the motivation of humans will flag catastrophically
in the face of this kind of technology. What you'll get is kind of the same thing that the
internet produced, which is little bubbles of incredible human sophistication. Look on YouTube
for young guitar players or young piano players or young violinists and look at the broad range of
solo violin players or something you could find there and sort of sample through that and you'll
see there's just a really diverse and rather large cohort of extreme performance skill
and peculiarly developed in every branch from putting things together made out of wood to playing
the piano to singing an acapella song, all these things, dancing, jumping, running, fighting,
everything, everything, everything, everything, everywhere all at once as they said, sort of.
So, you know, even though the majority of the humans you see probably seem relatively uninteresting,
there are among the humans these pinnacles of very different localization of skill,
passion, curiosity, wonder, intelligence, even rationality or something resembling computation.
And that will probably continue, but it will become much more sparse
and there will be a million or, you know, an endless number of pretenders, right, because
it won't matter who sees you on the internet if AIs can produce you playing your guitar better
than you do. All of these motivating factors that are crucially important to human
self-development and that get naturally emphasized in healthy communal groups but
fail dramatically in many, you know, isolated or very small groups
or individuals, right?
What will motivate the humans to become, to continue their development in the face of
a machine that can do most of what you can, almost everything you can or can appear to have done it?
There's one more little feature, but it's evading
my intelligence for the moment, so when it comes to me, if it comes, I will
add it in the recording notes. So much more to learn and see here. This is just a very
cursory overview of some of the mountaintops that immediately attract my concern and attention.
Actually dealing with the technology and being human in the face of it is
a very confusing thing. I found some of my interactions with Bard around image creation
quite intoxicating in the sense of actually intoxicating me.
I couldn't stop laughing and the implications that I could see in the complex images formed
by Bard around my prompt, the reflection of both the possibility of beauty and the
object insanity of producing a derivative sum over the space in images. That's
visually apparent in the image that this is going on. It's a variety of visual summing over the space
behaviors. And seeing that, undirected by an actual intelligence, or I better hope those
weren't directed by an actual intelligence, was like doing psychedelic drugs or something.
Really crazy, amazing, and strange, parahipnotic, very dangerous.
We will continue to learn and grow and see while we can together, and hopefully that will be
many generations to come for our people and the living beings of earth. But at the moment in
this part of the story, things look pretty damn fraught from here. Let us continue our lives and
creative endeavors with and for each other and the spirit of the history and future of
life on earth and intelligence in the universe, not just here. Perhaps our species is not quite as
alone as our technologies and languages pose us as being. And the use of perhaps in that sentence was
unjustified. Thank you for joining me. I look forward to learning again together sometime very soon.
Bye bye for now.
